import re
import requests
import base64
import socket
import os
import time
import json
import threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import maxminddb

# --- CONFIGURATION (MEGA SOURCES) ---
SOURCES = [

    "https://raw.githubusercontent.com/V2Ray-Flags/V2Ray-Flags/main/V2Ray-Flags.txt"
]

# File paths
PERSONAL_LINKS_FILE = "my_personal_links.txt"
ACTIVITY_LOG = "activity_log.txt"
OUTPUT_FILE = "my_stable_configs.txt"
BY_FILE = "BY_stable.txt"
KZ_FILE = "KZ_stable.txt"
CACHE_FILE = "proxy_cache.json"
STATUS_FILE = "status.json"

# Target countries (Elite Filter)
TARGET_COUNTRIES = ['BY', 'KZ', 'PL', 'CH', 'SE', 'DE', 'US', 'GB', 'FI', 'TR', 'NL', 'FR']

# Emoji Flags Dictionary
COUNTRY_FLAGS = {
    'BY': 'üáßüáæ', 'KZ': 'üá∞üáø', 'PL': 'üáµüá±', 'CH': 'üá®üá≠', 'SE': 'üá∏üá™', 
    'DE': 'üá©üá™', 'US': 'üá∫üá∏', 'GB': 'üá¨üáß', 'FI': 'üá´üáÆ', 'TR': 'üáπüá∑', 
    'NL': 'üá≥üá±', 'FR': 'üá´üá∑', 'UN': 'üåê'
}

# GeoIP settings
GEOIP_URL = "https://github.com/P3TERX/GeoLite.mmdb/raw/download/GeoLite2-Country.mmdb"
GEOIP_FILENAME = "GeoLite2-Country.mmdb"

# Performance settings
THREADS = 150 
TIMEOUT = 2.5 

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Ä–µ–∞–ª-—Ç–∞–π–º –æ—Ç—á–µ—Ç–∞
stats_lock = threading.Lock()
processed_count = 0
total_configs_to_check = 0
alive_found = 0
dead_found = 0
skipped_cache = 0
dns_fail = 0
wrong_country = 0

# --- SMART CACHE LOGIC ---
def load_cache():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –ø—Ä–æ–∫—Å–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π 3-–¥–Ω–µ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞."""
    if not os.path.exists(CACHE_FILE):
        print(f"[CACHE] üÜï –§–∞–π–ª {CACHE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π.")
        return {"start_date": datetime.now().isoformat(), "data": {}}
    
    try:
        with open(CACHE_FILE, 'r') as f:
            cache = json.load(f)
        
        start_date = datetime.fromisoformat(cache.get("start_date", datetime.now().isoformat()))
        if datetime.now() - start_date > timedelta(days=3):
            print("[CACHE] üîÑ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω (3 –¥–Ω—è). –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–π –ø–∞–º—è—Ç–∏...")
            return {"start_date": datetime.now().isoformat(), "data": {}}
            
        return cache
    except Exception as e:
        print(f"[CACHE] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return {"start_date": datetime.now().isoformat(), "data": {}}

def save_cache(cache_data):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø—Ä–æ–∫—Å–∏ –≤ –∫—ç—à."""
    try:
        with open(CACHE_FILE, 'w') as f:
            json.dump(cache_data, f, indent=2)
            f.flush()
        print(f"‚úÖ [CACHE] –ü–∞–º—è—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ in {CACHE_FILE}")
    except Exception as e:
        print(f"[CACHE] ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# --- CORE FUNCTIONS ---
def download_geoip_with_retry(retries=3):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –±–∞–∑—ã GeoIP —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏."""
    if os.path.exists(GEOIP_FILENAME):
        print("‚úÖ [GEOIP] –ë–∞–∑–∞ —É–∂–µ –Ω–∞ –º–µ—Å—Ç–µ.")
        return True
    
    for i in range(retries):
        try:
            print(f"üåê [GEOIP] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã (–ü–æ–ø—ã—Ç–∫–∞ {i+1})...")
            response = requests.get(GEOIP_URL, stream=True, timeout=30)
            response.raise_for_status()
            with open(GEOIP_FILENAME, 'wb') as f:
                f.write(response.content)
            print("‚úÖ [GEOIP] –ë–∞–∑–∞ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞.")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è [GEOIP] –°–±–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            if i < retries - 1:
                time.sleep(5)
    return False

def get_ip_from_host(host):
    """–†–µ–∑–æ–ª–≤–∏–Ω–≥ –¥–æ–º–µ–Ω–∞ –≤ IP –∞–¥—Ä–µ—Å."""
    try:
        clean_host = host.strip()
        if re.match(r"^\d{1,3}(\.\d{1,3}){3}$", clean_host):
            return clean_host
        return socket.gethostbyname(clean_host)
    except:
        return None

def check_tcp_port(ip, port):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ TCP –ø–æ—Ä—Ç–∞."""
    try:
        family = socket.AF_INET6 if ":" in ip else socket.AF_INET
        with socket.socket(family, socket.SOCK_STREAM) as s:
            s.settimeout(TIMEOUT)
            s.connect((ip, int(port)))
            return True
    except:
        return False

def extract_host_port(config):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è VLESS, VMess, Trojan, SS."""
    try:
        if config.startswith("vmess://"):
            vmess_data = config.replace("vmess://", "")
            padding = len(vmess_data) % 4
            if padding: vmess_data += "=" * (4 - padding)
            try:
                decoded_js = json.loads(base64.b64decode(vmess_data).decode('utf-8'))
                host = decoded_js.get('add')
                port = decoded_js.get('port')
                if host and port:
                    return str(host).strip(), str(port).strip(), "VMESS"
            except: pass

        if "@" in config:
            protocol = config.split("://")[0].upper()
            address_part = config.split("@")[1].split("?")[0].split("#")[0].split("/")[0]
            
            if address_part.startswith("["):
                match = re.search(r"\[(.+)\]:(\d+)", address_part)
                if match:
                    return match.group(1), match.group(2), protocol
            
            if ":" in address_part:
                parts = address_part.split(":")
                return parts[0].strip(), parts[-1].strip(), protocol

        elif config.startswith("ss://"):
            encoded_part = config.replace("ss://", "").split("#")[0]
            if ":" in encoded_part and "@" not in encoded_part: 
                 parts = encoded_part.split(":")
                 return parts[0].strip(), parts[1].strip(), "SS"
            
            padding = len(encoded_part) % 4
            if padding: encoded_part += "=" * (4 - padding)
            try:
                decoded = base64.b64decode(encoded_part).decode('utf-8', errors='ignore')
                if "@" in decoded:
                    address_part = decoded.split("@")[1].split("/")[0]
                    if ":" in address_part:
                        host, port = address_part.split(":")[:2]
                        return host.strip(), port.strip(), "SS"
            except: pass
    except: pass
    return None, None, "UNKNOWN"

def decode_content(content):
    """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ Base64 —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–æ–¥–ø–∏—Å–∫–∏."""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–Ω—è—Ç—å, —ç—Ç–æ Base64 –∏–ª–∏ —É–∂–µ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ç–µ–∫—Å—Ç
        if "://" not in content[:50]:
            return base64.b64decode(content).decode('utf-8')
    except: pass
    return content

def process_config(config, reader, cached_data):
    """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞."""
    global processed_count, alive_found, dead_found, skipped_cache, dns_fail, wrong_country
    
    config = config.strip()
    if not config or "://" not in config: return None

    # –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è BY –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    is_claimed_by = "BY" in config.upper() or "BELARUS" in config.upper()
    
    host, port, proto = extract_host_port(config)
    if not host or not port: return None

    fingerprint = f"{host}:{port}:{proto}"
    
    # 1. DNS –†–µ–∑–æ–ª–≤–∏–Ω–≥
    ip = get_ip_from_host(host)
    if not ip: 
        with stats_lock: 
            processed_count += 1
            dns_fail += 1
        progress = (processed_count / total_configs_to_check) * 100
        if is_claimed_by:
            print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [TRACE_BY] DNS –°–ë–û–ô: –ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ IP –¥–ª—è {host}")
        else:
            print(f"üö´ [{progress:.1f}%] [DNS_FAIL] {host} -> 0")
        return None

    # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –°–¢–†–û–ì–û –ø–æ IP
    try:
        geo_data = reader.get(ip)
        country_code = str(geo_data.get('country', {}).get('iso_code', 'UN')).upper()
    except:
        country_code = "UN"

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    if fingerprint in cached_data:
        if cached_data[fingerprint]["status"] == "dead":
            with stats_lock: 
                processed_count += 1
                skipped_cache += 1
            progress = (processed_count / total_configs_to_check) * 100
            print(f"üíæ [{progress:.1f}%] [CACHE_SKIP] {ip}:{port} -> 0")
            return {"status": "skipped"}

    # 4. –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
    if country_code not in TARGET_COUNTRIES:
        with stats_lock: 
            processed_count += 1
            wrong_country += 1
        progress = (processed_count / total_configs_to_check) * 100
        
        if is_claimed_by:
            print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è [TRACE_BY] –ì–ï–û –°–ë–û–ô: –í –∫–æ–Ω—Ñ–∏–≥–µ BY, –∞ –ø–æ –±–∞–∑–µ IP ({ip}) —ç—Ç–æ {country_code}")
        else:
            print(f"üåç [{progress:.1f}%] [WRONG_GEO] {country_code} | {ip}:{port} -> 0")
        return None
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ TCP –ø–æ—Ä—Ç–∞
    is_alive = check_tcp_port(ip, port)
    
    with stats_lock:
        processed_count += 1
        if is_alive: alive_found += 1
        else: dead_found += 1
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏
    cached_data[fingerprint] = {
        "status": "alive" if is_alive else "dead",
        "time": datetime.now().isoformat(),
        "ip": ip,
        "country": country_code
    }

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    progress = (processed_count / total_configs_to_check) * 100
    if is_alive:
        print(f"‚ú® [{progress:.1f}%] [FOUND] {country_code} | {proto} | {ip}:{port}")
    else:
        print(f"‚ùå [{progress:.1f}%] [DEAD] {country_code} | {proto} | {ip}:{port} -> 0")

    if not is_alive: 
        return None

    # 6. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
    flag = COUNTRY_FLAGS.get(country_code, 'üåê')
    base_url = config.split("#")[0]
    final_name = f"{flag} [{country_code}] {proto} | {ip}"
    
    return {
        "id": fingerprint, 
        "country": country_code, 
        "data": f"{base_url}#{final_name}",
        "status": "success"
    }

def update_activity_log(found, skipped, dead, dns, geo):
    """–ó–∞–ø–∏—Å—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –ª–æ–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."""
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(ACTIVITY_LOG, "a", encoding="utf-8") as f:
            log_line = (f"[{now}] –ñ–∏–≤—ã—Ö: {found} | –ú–µ—Ä—Ç–≤—ã—Ö: {dead} | –ö—ç—à: {skipped} | "
                        f"DNS_Fail: {dns} | Wrong_Geo: {geo}\n")
            f.write(log_line)
    except: pass

def main():
    global total_configs_to_check, processed_count, alive_found, dead_found, skipped_cache, dns_fail, wrong_country
    
    print("üöÄ --- MEGA WORKER V4.4 [FULL TRACE MODE] ---")
    start_time = time.time()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
    if not download_geoip_with_retry(): return

    reader = maxminddb.open_database(GEOIP_FILENAME)
    cache = load_cache()
    cached_data = cache["data"]
    initial_cache_size = len(cached_data)
    
    try:
        all_raw_configs = []
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–±–ª–∞—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print(f"üì° --- –§–ê–ó–ê –°–ë–û–†–ê: {len(SOURCES)} –ò–°–¢–û–ß–ù–ò–ö–û–í ---")
        for idx, url in enumerate(SOURCES, 1):
            try:
                start_fetch = time.time()
                r = requests.get(url, timeout=15)
                decoded = decode_content(r.text)
                lines = [l.strip() for l in decoded.splitlines() if l.strip()]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–æ–∫—Å–∏ —Å—Å—ã–ª–∫–∏
                valid_links = [l for l in lines if "://" in l]
                
                all_raw_configs.extend(valid_links)
                
                fetch_time = time.time() - start_fetch
                print(f"üîó [{idx:02}] {url[:60]}... | –ù–∞–π–¥–µ–Ω–æ: {len(valid_links)} (–∏–∑ {len(lines)} —Å—Ç—Ä–æ–∫) | {fetch_time:.1f}s")
            except Exception as e:
                print(f"‚ùå [{idx:02}] –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {url[:40]}: {str(e)[:50]}")

        # –°–±–æ—Ä –∏–∑ –ª–∏—á–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        if os.path.exists(PERSONAL_LINKS_FILE):
            print(f"\nüìñ --- –§–ê–ó–ê –°–ë–û–†–ê: –õ–ò–ß–ù–´–ï –°–°–´–õ–ö–ò ({PERSONAL_LINKS_FILE}) ---")
            with open(PERSONAL_LINKS_FILE, "r", encoding="utf-8") as f:
                personal_lines = f.read().splitlines()
                personal_count = 0
                for line in personal_lines:
                    line = line.strip()
                    if not line or line.startswith("#"): continue
                    
                    if line.startswith("http"):
                        try:
                            r = requests.get(line, timeout=15)
                            content = decode_content(r.text)
                            links = [l.strip() for l in content.splitlines() if "://" in l]
                            all_raw_configs.extend(links)
                            personal_count += len(links)
                            print(f"üìÅ –ü–æ–¥–ø–∏—Å–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞: {line[:50]}... | –ù–∞–π–¥–µ–Ω–æ: {len(links)}")
                        except: pass
                    else: 
                        all_raw_configs.append(line)
                        personal_count += 1
                print(f"‚úÖ –ò—Ç–æ–≥–æ –∏–∑ –ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {personal_count}")

        raw_count = len(all_raw_configs)
        unique_candidates = list(set(all_raw_configs))
        duplicates_count = raw_count - len(unique_candidates)
        total_configs_to_check = len(unique_candidates)
        
        print(f"\nüì¶ --- –ò–¢–û–ì–ò –°–ë–û–†–ê ---")
        print(f"üì¶ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫-—Å—Å—ã–ª–æ–∫: {raw_count}")
        print(f"üëØ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count}")
        print(f"üìä –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –≤ –∫—ç—à–µ: {initial_cache_size} –∑–∞–ø–∏—Å–µ–π")
        print(f"üîç –ö –ø—Ä–æ–≤–µ—Ä–∫–µ (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö): {total_configs_to_check}")
        
        results_list = []
        seen_ids = set()
        
        print(f"\nüõ†Ô∏è  –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ {THREADS} –ø–æ—Ç–æ–∫–æ–≤...")
        with ThreadPoolExecutor(max_workers=THREADS) as executor:
            future_tasks = [executor.submit(process_config, cfg, reader, cached_data) for cfg in unique_candidates]
            for future in as_completed(future_tasks):
                res = future.result()
                if res and res.get("status") == "success" and res['id'] not in seen_ids:
                    seen_ids.add(res['id'])
                    results_list.append(res)

        # –§–∞–∑–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ –∑–∞–ø–∏—Å–∏
        results_list.sort(key=lambda x: x['country'])
        
        by_configs = [r['data'] for r in results_list if r['country'] == 'BY']
        kz_configs = [r['data'] for r in results_list if r['country'] == 'KZ']
        other_configs = [r['data'] for r in results_list if r['country'] not in ['BY', 'KZ']]
        all_configs = [r['data'] for r in results_list]

        print("\nüèÅ --- –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ó–ê–ü–ò–°–ò ---")
        
        def safe_write(filename, data_list):
            try:
                with open(filename, "w", encoding="utf-8") as f:
                    f.write("\n".join(data_list))
                    f.flush()
                    os.fsync(f.fileno())
                print(f"üíæ [FILE] {filename:18} | –ó–∞–ø–∏—Å–∞–Ω–æ: {len(data_list):4} —à—Ç.")
            except Exception as e:
                print(f"‚ùå [ERROR] –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ {filename}: {e}")

        safe_write(OUTPUT_FILE, all_configs)
        safe_write(BY_FILE, by_configs)
        safe_write(KZ_FILE, kz_configs)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–ª—è UI/Actions
        status_data = {
            "last_run": datetime.now().isoformat(),
            "total_alive": len(all_configs),
            "by": len(by_configs),
            "kz": len(kz_configs),
            "cache_skipped": skipped_cache,
            "dead_total": dead_found,
            "dns_fail": dns_fail,
            "wrong_geo": wrong_country,
            "initial_cache_size": initial_cache_size,
            "duplicates_removed": duplicates_count
        }
        with open(STATUS_FILE, "w") as f:
            json.dump(status_data, f)

        update_activity_log(len(all_configs), skipped_cache, dead_found, dns_fail, wrong_country)
        
        duration = time.time() - start_time
        print(f"\nüìä –°–£–ú–ú–ê–†–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"‚úÖ –í—Å–µ–≥–æ –∂–∏–≤—ã—Ö: {len(all_configs)}")
        print(f"üáßüáæ –ë–µ–ª–∞—Ä—É—Å—å (BY): {len(by_configs)}")
        print(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω (KZ): {len(kz_configs)}")
        print(f"üåç –î—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω—ã: {len(other_configs)}")
        print(f"------------------------------------")
        print(f"‚ùå –ú–µ—Ä—Ç–≤—ã—Ö (TCP): {dead_found}")
        print(f"üíæ –ö—ç—à (–°–∫–∏–ø): {skipped_cache}")
        print(f"üåê DNS –û—à–∏–±–∫–∏: {dns_fail}")
        print(f"üö´ –ù–µ—Ü–µ–ª–µ–≤—ã–µ –ì–ï–û: {wrong_country}")
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count} –∏–∑ {total_configs_to_check}")
        print(f"‚è±Ô∏è  –í–†–ï–ú–Ø –†–ê–ë–û–¢–´: {duration:.1f} —Å–µ–∫.")

    except Exception as e:
        print(f"üö® [FATAL ERROR] {e}")
    finally:
        save_cache(cache)
        reader.close()

if __name__ == "__main__":
    main()
